name: Recon Autonomy Agent

on:
  schedule:
    - cron: '0 0 * * *' # Executa diariamente Ã  meia-noite
  workflow_dispatch: # Permite execuÃ§Ã£o manual

jobs:
  recon-mission:
    runs-on: ubuntu-latest
    permissions:
      contents: write # PermissÃ£o para commitar os resultados

    steps:
    - name: Checkout Code
      uses: actions/checkout@v3

    - name: Setup Go
      uses: actions/setup-go@v4
      with:
        go-version: '1.21'

    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'

    - name: Install Recon Tools
      run: |
        echo "Installing GAU..."
        go install github.com/lc/gau/v2/cmd/gau@latest
        echo "Installing Waybackurls..."
        go install github.com/tomnomnom/waybackurls@latest
        echo "Installing Hakrawler..."
        go install github.com/hakluke/hakrawler@latest
        echo "Installing Katana..."
        go install github.com/projectdiscovery/katana/cmd/katana@latest
        # Adicionar ao PATH
        export PATH=$PATH:$(go env GOPATH)/bin
        echo "$(go env GOPATH)/bin" >> $GITHUB_PATH

    - name: Run Deep Scan
      env:
        TARGET_DOMAIN: "primos.mat.br"
      run: |
        mkdir -p temp_recon
        echo "Iniciando varredura passiva e ativa (V12) em $TARGET_DOMAIN..."
        
        # 1. GAU (Get All Urls - HistÃ³rico)
        echo "Executando GAU..."
        echo $TARGET_DOMAIN | gau --threads 5 > temp_recon/gau_raw.txt || true
        
        # 2. Waybackurls (Backup)
        echo "Executando Waybackurls..."
        echo $TARGET_DOMAIN | waybackurls > temp_recon/wayback_raw.txt || true

        # 3. Katana (Active JS Crawling)
        echo "Executando Katana..."
        katana -u "https://$TARGET_DOMAIN" -d 2 -jc -o temp_recon/katana_raw.txt || true
        
        # 4. Merge e Limpeza
        cat temp_recon/gau_raw.txt temp_recon/wayback_raw.txt temp_recon/katana_raw.txt | sort -u > temp_recon/all_urls.txt
        
        # 5. Filtros Sniper (V12)
        # Limpar lixo estÃ¡tico (grep -vE)
        grep -vE "\.(jpg|jpeg|gif|css|tif|tiff|png|ttf|woff|woff2|ico|pdf|svg|txt|js|xml)$" temp_recon/all_urls.txt > temp_recon/clean_urls.txt || true
        
        # Extrair parÃ¢metros interessantes (Sniper)
        grep -E "(\?|&)(id|page|user|token|key|admin|debug)=" temp_recon/clean_urls.txt > temp_recon/params_urls.txt || true
        
        echo "URLs Totais: $(wc -l < temp_recon/clean_urls.txt)"
        echo "URLs com ParÃ¢metros: $(wc -l < temp_recon/params_urls.txt)"

    - name: Process Data to JSON
      run: |
        # Script simples para converter a lista de URLs em JSON estruturado
        # Em um cenÃ¡rio real, usarÃ­amos um script Python/Node mais robusto para extrair tÃ­tulos via requests
        
        mkdir -p public/data
        
        echo "{" > public/data/recon_results.json
        echo "  \"lastScan\": \"$(date -u +"%Y-%m-%dT%H:%M:%SZ")\"," >> public/data/recon_results.json
        echo "  \"totalLinks\": $(wc -l < temp_recon/clean_urls.txt)," >> public/data/recon_results.json
        echo "  \"status\": \"Ativo\"," >> public/data/recon_results.json
        echo "  \"recoveredArticles\": [" >> public/data/recon_results.json
        
        # Pega as primeiras 20 URLs como exemplo/amostra para o JSON
        count=0
        while read p; do
          if [ $count -gt 0 ]; then
            echo "," >> public/data/recon_results.json
          fi
          # Escapar aspas duplas na URL
          clean_url=$(echo "$p" | sed 's/"/\\"/g')
          # Simular fonte baseada na ferramenta (apenas para demo, na real viria do output)
          echo "    { \"url\": \"$clean_url\", \"title\": \"Recuperado via AutomaÃ§Ã£o\", \"source\": \"Wayback Machine\" }" >> public/data/recon_results.json
          count=$((count+1))
          if [ $count -eq 20 ]; then break; fi
        done < temp_recon/clean_urls.txt
        
        echo "  ]" >> public/data/recon_results.json
        echo "}" >> public/data/recon_results.json
        
        cat public/data/recon_results.json

    - name: Install dependencies
      run: npm ci

    - name: Push Results to Database (Supabase)
      if: success()
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
      run: npm run db:push
      
    - name: Commit and Push JSON Fallback
      run: |
        git config --global user.name 'Recon Bot'
        git config --global user.email 'bot@primos.mat.br'
        git add public/data/recon_results.json
        git commit -m "Auto-update: Recon Scan Results [$(date)]" || exit 0
        git push

    - name: Notify Telegram (Success)
      if: success()
      run: |
        node scripts/notify-telegram.js "âœ… RECON COMPLETO\n\nNovos Links Encontrados: $(wc -l < temp_recon/clean_urls.txt)\nPotenciais Alvos (Sniper): $(wc -l < temp_recon/params_urls.txt)\n\nRelatÃ³rio atualizado no Dashboard." "SUCCESS"

    - name: Notify Telegram (Failure)
      if: failure()
      run: |
        node scripts/notify-telegram.js "ðŸš¨ FALHA NO RECON\n\nO agente de varredura encontrou um erro crÃ­tico durante a execuÃ§Ã£o. Verifique os logs do GitHub Actions imediatamente." "ALERT"
